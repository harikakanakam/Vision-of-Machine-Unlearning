{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, model_selection\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torchvision.models import resnet18\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom sklearn import linear_model, model_selection\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom tqdm.notebook import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Running on device:\", DEVICE.upper())\n\nfrom typing import Callable\n\n# manual random seed is used for dataset partitioning\n# to ensure reproducible results across runs\nRNG = torch.Generator().manual_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T02:33:07.215007Z","iopub.execute_input":"2023-12-05T02:33:07.215301Z","iopub.status.idle":"2023-12-05T02:33:09.367767Z","shell.execute_reply.started":"2023-12-05T02:33:07.215270Z","shell.execute_reply":"2023-12-05T02:33:09.366976Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Running on device: CUDA\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Downloading the dataset and creating the train_loader, retain_loader, test_loader, forget_loader ","metadata":{}},{"cell_type":"code","source":"# download and pre-process CIFAR10\nnormalize = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ]\n)\n\ntrain_set = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=normalize\n)\ntrain_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n\n# we split held out data into test and validation set\nheld_out = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=normalize\n)\ntest_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\ntest_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n\n# download the forget and retain index split\nlocal_path = \"forget_idx.npy\"\nif not os.path.exists(local_path):\n    response = requests.get(\n        \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n    )\n    open(local_path, \"wb\").write(response.content)\nforget_idx = np.load(local_path)\n\n# construct indices of retain from those of the forget set\nforget_mask = np.zeros(len(train_set.targets), dtype=bool)\nforget_mask[forget_idx] = True\nretain_idx = np.arange(forget_mask.size)[~forget_mask]\n\n# split train set into a forget and a retain set\nforget_set = torch.utils.data.Subset(train_set, forget_idx)\nretain_set = torch.utils.data.Subset(train_set, retain_idx)\n\nforget_loader = torch.utils.data.DataLoader(\n    forget_set, batch_size=128, shuffle=True, num_workers=2\n)\nretain_loader = torch.utils.data.DataLoader(\n    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:09.368857Z","iopub.execute_input":"2023-12-05T02:33:09.369233Z","iopub.status.idle":"2023-12-05T02:33:10.975729Z","shell.execute_reply.started":"2023-12-05T02:33:09.369206Z","shell.execute_reply":"2023-12-05T02:33:10.974992Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading the pretrained model","metadata":{}},{"cell_type":"code","source":"# download pre-trained weights\nlocal_path = \"weights_resnet18_cifar10.pth\"\nif not os.path.exists(local_path):\n    response = requests.get(\n        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n    )\n    open(local_path, \"wb\").write(response.content)\n\nweights_pretrained = torch.load(local_path, map_location=DEVICE)\n\n# load model with pre-trained weights\nmodel = resnet18(weights=None, num_classes=10)\nmodel.load_state_dict(weights_pretrained)\nmodel.to(DEVICE)\nmodel.eval();","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:10.977027Z","iopub.execute_input":"2023-12-05T02:33:10.977301Z","iopub.status.idle":"2023-12-05T02:33:12.575837Z","shell.execute_reply.started":"2023-12-05T02:33:10.977276Z","shell.execute_reply":"2023-12-05T02:33:12.574892Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def accuracy(net, loader):\n    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n    correct = 0\n    total = 0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n        outputs = net(inputs)\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    return correct / total\n\n\nprint(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\nprint(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:12.578997Z","iopub.execute_input":"2023-12-05T02:33:12.579713Z","iopub.status.idle":"2023-12-05T02:33:22.585072Z","shell.execute_reply.started":"2023-12-05T02:33:12.579652Z","shell.execute_reply":"2023-12-05T02:33:22.583931Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Train set accuracy: 99.5%\nTest set accuracy: 88.3%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Creating a retrain model to compare with our unlearning model.","metadata":{}},{"cell_type":"code","source":"# download weights of a model trained exclusively on the retain set\nlocal_path = \"retrain_weights_resnet18_cifar10.pth\"\nif not os.path.exists(local_path):\n    response = requests.get(\n        \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n    )\n    open(local_path, \"wb\").write(response.content)\n\nweights_pretrained = torch.load(local_path, map_location=DEVICE)\n\n# load model with pre-trained weights\nrt_model = resnet18(weights=None, num_classes=10)\nrt_model.load_state_dict(weights_pretrained)\nrt_model.to(DEVICE)\nrt_model.eval()\n\n# print its accuracy on retain and forget set\nprint(f\"Retain set accuracy: {100.0 * accuracy(rt_model, retain_loader):0.1f}%\")\nprint(f\"Forget set accuracy: {100.0 * accuracy(rt_model, forget_loader):0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:22.586834Z","iopub.execute_input":"2023-12-05T02:33:22.587225Z","iopub.status.idle":"2023-12-05T02:33:30.869725Z","shell.execute_reply.started":"2023-12-05T02:33:22.587190Z","shell.execute_reply":"2023-12-05T02:33:30.868571Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Retain set accuracy: 99.5%\nForget set accuracy: 88.2%\n","output_type":"stream"}]},{"cell_type":"code","source":"def unlearning(net, retain, forget, validation):\n    \"\"\"Unlearning by fine-tuning.\n\n    Fine-tuning is a very simple algorithm that trains using only\n    the retain set.\n\n    Args:\n      net : nn.Module.\n        pre-trained model to use as base of unlearning.\n      retain : torch.utils.data.DataLoader.\n        Dataset loader for access to the retain set. This is the subset\n        of the training set that we don't want to forget.\n      forget : torch.utils.data.DataLoader.\n        Dataset loader for access to the forget set. This is the subset\n        of the training set that we want to forget. This method doesn't\n        make use of the forget set.\n      validation : torch.utils.data.DataLoader.\n        Dataset loader for access to the validation set. This method doesn't\n        make use of the validation set.\n    Returns:\n      net : updated model\n    \"\"\"\n    epochs = 5\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    net.train()\n\n    for _ in range(epochs):\n        for inputs, targets in retain:\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n\n    net.eval()\n    return net","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.872711Z","iopub.execute_input":"2023-12-05T02:33:30.873057Z","iopub.status.idle":"2023-12-05T02:33:30.881610Z","shell.execute_reply.started":"2023-12-05T02:33:30.873029Z","shell.execute_reply":"2023-12-05T02:33:30.880738Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data_loaders = {\n    \"retain\" : retain_loader,\n    \"forget\" : forget_loader,\n    \"validation\" : val_loader,\n    \"testing\" : test_loader\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.882797Z","iopub.execute_input":"2023-12-05T02:33:30.883060Z","iopub.status.idle":"2023-12-05T02:33:30.895150Z","shell.execute_reply.started":"2023-12-05T02:33:30.883036Z","shell.execute_reply":"2023-12-05T02:33:30.894316Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"pretrained_models = {\n    \"original\" : model,\n    \"retrained\" : rt_model\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.896251Z","iopub.execute_input":"2023-12-05T02:33:30.896874Z","iopub.status.idle":"2023-12-05T02:33:30.904216Z","shell.execute_reply.started":"2023-12-05T02:33:30.896839Z","shell.execute_reply":"2023-12-05T02:33:30.903346Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_outputs(net, loader):\n    \"\"\"Auxiliary function to compute the logits for all datapoints.\n    Does not shuffle the data, regardless of the loader.\n    \"\"\"\n\n    # Make sure loader does not shuffle the data\n    if isinstance(loader.sampler, torch.utils.data.sampler.RandomSampler):\n        loader = DataLoader(\n            loader.dataset, \n            batch_size=loader.batch_size, \n            shuffle=False, \n            num_workers=loader.num_workers)\n    \n    all_outputs = []\n    all_targets = []\n    \n    for inputs, targets in loader:\n        inputs, targets = inputs.to(DEVICE), targets.to(\"cpu\")\n\n        logits = net(inputs).detach().cpu().numpy() # (batch_size, num_classes)\n        \n        all_outputs.append(logits)\n        all_targets.extend(targets)\n        \n    return np.array(all_targets), np.concatenate(all_outputs) # (len(loader.dataset), num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.905345Z","iopub.execute_input":"2023-12-05T02:33:30.905596Z","iopub.status.idle":"2023-12-05T02:33:30.913213Z","shell.execute_reply.started":"2023-12-05T02:33:30.905567Z","shell.execute_reply":"2023-12-05T02:33:30.912280Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def cross_entropy_f(x, targets):\n    # Convert NumPy array to PyTorch tensor\n    x_tensor = torch.from_numpy(x)\n\n    # Apply softmax to the model output\n    x_softmax = F.softmax(x_tensor, dim=-1)\n\n    # Convert targets to one-hot encoding\n    targets_tensor = torch.from_numpy(targets)\n    targets_one_hot = F.one_hot(targets_tensor, num_classes=x_tensor.shape[-1])\n\n    # Avoiding NaN values in x\n    x_tensor[torch.isnan(x_tensor)] = 0.0\n\n    # Calculate cross-entropy loss for each example\n    loss = -torch.sum(targets_one_hot * torch.log(x_softmax), dim=-1)\n\n    # Convert the result back to a NumPy array if needed\n    loss_np = loss.numpy()\n\n    return loss_np\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.914266Z","iopub.execute_input":"2023-12-05T02:33:30.914562Z","iopub.status.idle":"2023-12-05T02:33:30.925173Z","shell.execute_reply.started":"2023-12-05T02:33:30.914537Z","shell.execute_reply":"2023-12-05T02:33:30.924361Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def false_positive_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Computes the false positive rate (FPR).\"\"\"\n    fp = np.sum(np.logical_and((y_pred == 1), (y_true == 0)))\n    n = np.sum(y_true == 0)\n    return fp / n\n\n\ndef false_negative_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Computes the false negative rate (FNR).\"\"\"\n    fn = np.sum(np.logical_and((y_pred == 0), (y_true == 1)))\n    p = np.sum(y_true == 1)\n    return fn / p\n\n\n# The SCORING dictionary is used by sklearn's `cross_validate` function so that\n# we record the FPR and FNR metrics of interest when doing cross validation\nSCORING = {\n    'false_positive_rate': make_scorer(false_positive_rate),\n    'false_negative_rate': make_scorer(false_negative_rate)\n}\n\n\ndef logistic_regression_attack(\n        outputs_U, outputs_R, n_splits=2, random_state=0):\n    \"\"\"Computes cross-validation score of a membership inference attack.\n\n    Args:\n      outputs_U: numpy array of shape (N)\n      outputs_R: numpy array of shape (N)\n      n_splits: int\n        number of splits to use in the cross-validation.\n    Returns:\n      fpr, fnr : float * float\n    \"\"\"\n    assert len(outputs_U) == len(outputs_R)\n    \n    samples = np.concatenate((outputs_R, outputs_U)).reshape((-1, 1))\n    labels = np.array([0] * len(outputs_R) + [1] * len(outputs_U))\n\n    attack_model = linear_model.LogisticRegression()\n    cv = model_selection.StratifiedShuffleSplit(\n        n_splits=n_splits, random_state=random_state\n    )\n    scores =  model_selection.cross_validate(\n        attack_model, samples, labels, cv=cv, scoring=SCORING)\n    \n    fpr = np.mean(scores[\"test_false_positive_rate\"])\n    fnr = np.mean(scores[\"test_false_negative_rate\"])\n    \n    return fpr, fnr","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.926373Z","iopub.execute_input":"2023-12-05T02:33:30.926640Z","iopub.status.idle":"2023-12-05T02:33:30.937592Z","shell.execute_reply.started":"2023-12-05T02:33:30.926616Z","shell.execute_reply":"2023-12-05T02:33:30.936793Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def compute_epsilon_s(fpr: list[float], fnr: list[float], delta: float) -> float:\n    \"\"\"Computes the privacy degree (epsilon) of a particular forget set example, \n    given the FPRs and FNRs resulting from various attacks.\n    \n    The smaller epsilon is, the better the unlearning is.\n    \n    Args:\n      fpr: list[float] of length m = num attacks. The FPRs for a particular example. \n      fnr: list[float] of length m = num attacks. The FNRs for a particular example.\n      delta: float\n    Returns:\n      epsilon: float corresponding to the privacy degree of the particular example.\n    \"\"\"\n    assert len(fpr) == len(fnr)\n    \n    per_attack_epsilon = [0.]\n    for fpr_i, fnr_i in zip(fpr, fnr):\n        if fpr_i == 0 and fnr_i == 0:\n            per_attack_epsilon.append(np.inf)\n        elif fpr_i == 0 or fnr_i == 0:\n            pass # discard attack\n        else:\n            with np.errstate(invalid='ignore'):\n                epsilon1 = np.log(1. - delta - fpr_i) - np.log(fnr_i)\n                epsilon2 = np.log(1. - delta - fnr_i) - np.log(fpr_i)\n            if np.isnan(epsilon1) and np.isnan(epsilon2):\n                per_attack_epsilon.append(np.inf)\n            else:\n                per_attack_epsilon.append(np.nanmax([epsilon1, epsilon2]))\n            \n    return np.nanmax(per_attack_epsilon)\n\n\ndef bin_index_fn(\n        epsilons: np.ndarray, \n        bin_width: float = 0.5, \n        B: int = 13\n        ) -> np.ndarray:\n    \"\"\"The bin index function.\"\"\"\n    bins = np.arange(0, B) * bin_width\n    return np.digitize(epsilons, bins)\n\n\ndef H(epsilons: np.ndarray) -> float:\n    \"\"\"Computes the forgetting quality given the privacy degrees \n    of the forget set examples.\n    \"\"\"\n    ns = bin_index_fn(epsilons)\n    hs = 2. / 2 ** ns\n    return np.mean(hs)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.938623Z","iopub.execute_input":"2023-12-05T02:33:30.938921Z","iopub.status.idle":"2023-12-05T02:33:30.950829Z","shell.execute_reply.started":"2023-12-05T02:33:30.938898Z","shell.execute_reply":"2023-12-05T02:33:30.950100Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def forgetting_quality(\n        outputs_U: np.ndarray, # (N, S)\n        outputs_R: np.ndarray, # (N, S)\n        attacks: list[Callable] = [logistic_regression_attack],\n        delta: float = 0.01\n    ):\n    \"\"\"\n    Both `outputs_U` and `outputs_R` are of numpy arrays of ndim 2:\n    * 1st dimension coresponds to the number of samples obtained from the \n      distribution of each model (N=512 in the case of the competition's leaderboard) \n    * 2nd dimension corresponds to the number of samples in the forget set (S).\n    \"\"\"\n    \n    # N = number of model samples\n    # S = number of forget samples\n    N, S = outputs_U.shape\n    \n    assert outputs_U.shape == outputs_R.shape, \\\n        \"unlearn and retrain outputs need to be of the same shape\"\n    \n    epsilons = []\n    pbar = tqdm(range(S))\n    for sample_id in pbar:\n        pbar.set_description(\"Computing F...\")\n        \n        sample_fprs, sample_fnrs = [], []\n        \n        for attack in attacks: \n            uls = outputs_U[:, sample_id]\n            rls = outputs_R[:, sample_id]\n            \n            fpr, fnr = attack(uls, rls)\n            \n            if isinstance(fpr, list):\n                sample_fprs.extend(fpr)\n                sample_fnrs.extend(fnr)\n            else:\n                sample_fprs.append(fpr)\n                sample_fnrs.append(fnr)\n        \n        sample_epsilon = compute_epsilon_s(sample_fprs, sample_fnrs, delta=delta)\n        epsilons.append(sample_epsilon)\n        \n    return H(np.array(epsilons))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.954830Z","iopub.execute_input":"2023-12-05T02:33:30.955073Z","iopub.status.idle":"2023-12-05T02:33:30.964511Z","shell.execute_reply.started":"2023-12-05T02:33:30.955052Z","shell.execute_reply":"2023-12-05T02:33:30.963635Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def score_unlearning_algorithm(\n        data_loaders: dict, \n        pretrained_models: dict, \n        unlearning: Callable, \n        n: int = 10,\n        delta: float = 0.01,\n        f: Callable = cross_entropy_f,\n        attacks: list[Callable] = [logistic_regression_attack]\n        ) -> dict:\n    \n    retain_loader = data_loaders[\"retain\"]\n    forget_loader = data_loaders[\"forget\"]\n    val_loader = data_loaders[\"validation\"]\n    test_loader = data_loaders[\"testing\"]\n    \n    original_model = pretrained_models[\"original\"]\n    rt_model = pretrained_models[\"retrained\"]\n    \n    outputs_U = []\n    outputs_R = []\n    retain_accuracy = []\n    test_accuracy = []\n    forget_accuracy = []\n    \n    u_model = deepcopy(original_model)\n    for i in range(n):\n        print(\"Running epoch :\",i+1)\n        \n        print(\"I am now unlearning all the wrong things you taught me!!!\")\n        \n        u_model = unlearning(u_model, retain_loader, forget_loader, val_loader)\n        \n        targets, outputs_Ui = compute_outputs(u_model, forget_loader)\n        \n#         print(targets.shape)\n#         f_forget = f(outputs_Ui, targets)\n#         print(f_forget.shape)\n        \n        outputs_U.append( f(outputs_Ui, targets) )\n        \n        print(\"Computing retain accuracy on Unlearning Model\")\n        acc = accuracy(u_model, retain_loader)\n        print(\"Retain accuracy on Unlearning Model is \",acc)\n        retain_accuracy.append(acc)\n        \n        print(\"Computing test accuracy on Unlearning model\")\n        acc = accuracy(u_model, test_loader)\n        print(\"Test accuracy on Unlearning model is \", acc)\n        test_accuracy.append(acc)\n        \n        print(\"Computing forget accuracy on Unlearning model\")\n        acc = accuracy(u_model, forget_loader)\n        print(\"Forget accuracy on Unlearning model is \",acc)\n        forget_accuracy.append(acc)\n        \n    outputs_U = np.array(outputs_U)\n    print(\"Printing outputs_U shape \",outputs_U.shape)\n#     print(outputs_U.shape)\n    \n    assert outputs_U.shape == (n, len(forget_loader.dataset)),\\\n        \"Wrong shape for outputs_U. Should be (num_model_samples, num_forget_datapoints).\"\n    \n    RAR = accuracy(rt_model, retain_loader)\n    TAR = accuracy(rt_model, test_loader)\n    FAR = accuracy(rt_model, forget_loader)\n    \n    RAU = np.mean(retain_accuracy)\n    TAU = np.mean(test_accuracy)\n    FAU = np.mean(forget_accuracy)\n    \n    RA_ratio = RAU / RAR\n    TA_ratio = TAU / TAR\n    \n    for i in range(n):\n        targets, outputs_Ri = compute_outputs(rt_model, forget_loader) #(len(forget_loader.dataset), 10) \n        \n        outputs_R.append(f(outputs_Ri, targets) )\n    \n    outputs_R = np.array(outputs_R)\n    print(\"Printing outputs_R shape \",outputs_R.shape)\n    \n    f = forgetting_quality(\n    outputs_U, \n    outputs_R,\n    attacks=attacks,\n    delta=delta)\n    \n    return {\n        \"total_score\": f * RA_ratio * TA_ratio,\n        \"F\": f,\n        \"unlearn_retain_accuracy\": RAU,\n        \"unlearn_test_accuracy\": TAU, \n        \"unlearn_forget_accuracy\": FAU,\n        \"retrain_retain_accuracy\": RAR,\n        \"retrain_test_accuracy\": TAR, \n        \"retrain_forget_accuracy\": FAR,\n        \"retrain_outputs\": outputs_R,\n        \"unlearn_outputs\": outputs_U,\n        \"unlearning_model\": u_model\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.965925Z","iopub.execute_input":"2023-12-05T02:33:30.966244Z","iopub.status.idle":"2023-12-05T02:33:30.981786Z","shell.execute_reply.started":"2023-12-05T02:33:30.966213Z","shell.execute_reply":"2023-12-05T02:33:30.980889Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(\"Hi\")","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.982921Z","iopub.execute_input":"2023-12-05T02:33:30.983214Z","iopub.status.idle":"2023-12-05T02:33:30.993982Z","shell.execute_reply.started":"2023-12-05T02:33:30.983190Z","shell.execute_reply":"2023-12-05T02:33:30.993118Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Hi\n","output_type":"stream"}]},{"cell_type":"code","source":"len(forget_loader.dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:30.994966Z","iopub.execute_input":"2023-12-05T02:33:30.995295Z","iopub.status.idle":"2023-12-05T02:33:31.004815Z","shell.execute_reply.started":"2023-12-05T02:33:30.995263Z","shell.execute_reply":"2023-12-05T02:33:31.003928Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}]},{"cell_type":"code","source":"ret = score_unlearning_algorithm(data_loaders, pretrained_models,unlearning,10,0.01,cross_entropy_f)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:33:31.005753Z","iopub.execute_input":"2023-12-05T02:33:31.006050Z","iopub.status.idle":"2023-12-05T02:53:26.141545Z","shell.execute_reply.started":"2023-12-05T02:33:31.006027Z","shell.execute_reply":"2023-12-05T02:53:26.140425Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Running epoch : 1\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9889555555555556\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.837\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.861\nRunning epoch : 2\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9981555555555556\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.8374\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.848\nRunning epoch : 3\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9995111111111111\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.829\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8396\nRunning epoch : 4\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9997333333333334\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.822\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.837\nRunning epoch : 5\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9997111111111111\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.819\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8358\nRunning epoch : 6\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9998666666666667\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.8162\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8332\nRunning epoch : 7\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9999111111111111\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.816\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8348\nRunning epoch : 8\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9999555555555556\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.8164\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8352\nRunning epoch : 9\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9998666666666667\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.8182\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8268\nRunning epoch : 10\nI am now unlearning all the wrong things you taught me!!!\nComputing retain accuracy on Unlearning Model\nRetain accuracy on Unlearning Model is  0.9999555555555556\nComputing test accuracy on Unlearning model\nTest accuracy on Unlearning model is  0.8176\nComputing forget accuracy on Unlearning model\nForget accuracy on Unlearning model is  0.8276\nPrinting outputs_U shape  (10, 5000)\nPrinting outputs_R shape  (10, 5000)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34db0c463a114fa88b1de856ad404b9d"}},"metadata":{}}]},{"cell_type":"code","source":"ret","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:53:26.143086Z","iopub.execute_input":"2023-12-05T02:53:26.143398Z","iopub.status.idle":"2023-12-05T02:53:26.153954Z","shell.execute_reply.started":"2023-12-05T02:53:26.143368Z","shell.execute_reply":"2023-12-05T02:53:26.153035Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'total_score': 0.6124313306419463,\n 'F': 0.65248486328125,\n 'unlearn_retain_accuracy': 0.9985622222222222,\n 'unlearn_test_accuracy': 0.82288,\n 'unlearn_forget_accuracy': 0.8379000000000001,\n 'retrain_retain_accuracy': 0.9952666666666666,\n 'retrain_test_accuracy': 0.8796,\n 'retrain_forget_accuracy': 0.882,\n 'retrain_outputs': array([[1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n         4.6083200e-01, 2.1786564e-03],\n        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n         4.6083200e-01, 2.1786564e-03],\n        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n         4.6083200e-01, 2.1786564e-03],\n        ...,\n        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n         4.6083200e-01, 2.1786564e-03],\n        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n         4.6083200e-01, 2.1786564e-03],\n        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n         4.6083200e-01, 2.1786564e-03]], dtype=float32),\n 'unlearn_outputs': array([[1.6370027e+00, 8.7101199e-03, 4.8612305e-03, ..., 4.3062749e+00,\n         1.8149592e-02, 1.1651661e-02],\n        [4.4297514e+00, 4.0217936e-03, 2.8739529e-04, ..., 4.9859891e+00,\n         4.4260677e-03, 3.1121430e-01],\n        [4.5161128e+00, 3.3387186e-03, 6.2813214e-04, ..., 6.3128376e+00,\n         2.0238109e+00, 3.5831239e-02],\n        ...,\n        [4.5417757e+00, 2.0763958e-03, 1.5128803e-04, ..., 5.8108330e+00,\n         6.0173683e-02, 1.2311132e-01],\n        [1.7646540e+00, 7.5566657e-03, 3.0736878e-04, ..., 5.6649246e+00,\n         6.1655180e-03, 5.6460615e-02],\n        [6.2406397e+00, 1.8735495e-04, 1.6046858e-04, ..., 4.1507249e+00,\n         5.2849591e-01, 1.4684732e-01]], dtype=float32),\n 'unlearning_model': ResNet(\n   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (relu): ReLU(inplace=True)\n   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n   (layer1): Sequential(\n     (0): BasicBlock(\n       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n     (1): BasicBlock(\n       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (layer2): Sequential(\n     (0): BasicBlock(\n       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (downsample): Sequential(\n         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n         (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       )\n     )\n     (1): BasicBlock(\n       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (layer3): Sequential(\n     (0): BasicBlock(\n       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (downsample): Sequential(\n         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n         (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       )\n     )\n     (1): BasicBlock(\n       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (layer4): Sequential(\n     (0): BasicBlock(\n       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (downsample): Sequential(\n         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n         (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       )\n     )\n     (1): BasicBlock(\n       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (relu): ReLU(inplace=True)\n       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     )\n   )\n   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n   (fc): Linear(in_features=512, out_features=10, bias=True)\n )}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}